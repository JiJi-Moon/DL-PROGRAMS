{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9750277-61da-43aa-8099-e3a9dc6b7142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'football': [-0.25216895 -0.28408045 -0.38735878 -0.19149698 -0.22130427  0.24655208\n",
      "  0.05013163  0.19267978 -0.03761662 -0.5207101  -0.12992077  0.02772531\n",
      "  0.12215419  0.33139762  0.25879356  0.31256044  0.19376655  0.10897477\n",
      "  0.19096015 -0.0152914  -0.28872964  0.21509966  0.29502094 -0.29329804\n",
      " -0.18732865 -0.5092227  -0.50656354  0.13034034  0.5461856  -0.23984843\n",
      " -0.22297353  0.3279536   0.49203598 -0.3975505  -0.15423676  0.1424103\n",
      " -0.13092676  0.12414695 -0.4346669  -0.42910096 -0.4170664  -0.16287827\n",
      "  0.13415605  0.23163262 -0.1866648  -0.14442295  0.06076226  0.01882784\n",
      "  0.26654524  0.3160233 ]\n",
      "Context words for 'playing': ['i', 'playing', 'like', 'football', 'with']\n"
     ]
    }
   ],
   "source": [
    "#3 Skip Gram Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "corpus = [\"I like playing football with my friends\",\n",
    "          \"I enjoy playing tennis\",\n",
    "          \"I hate swimming\",\n",
    "          \"I love basketball\"]\n",
    "window_size, embedding_dim, batch_size, epochs, lr = 3, 50, 16, 100, 0.01\n",
    "\n",
    "# Tokenize\n",
    "tok = tf.keras.preprocessing.text.Tokenizer()\n",
    "tok.fit_on_texts(corpus)\n",
    "seqs = tok.texts_to_sequences(corpus)\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "\n",
    "# Generate skip-gram pairs\n",
    "pairs = [[w, c] for seq in seqs for i, w in enumerate(seq)\n",
    "         for j in range(max(0, i - window_size), min(len(seq), i + window_size + 1)) if i != j and (c := seq[j])]\n",
    "pairs = np.array(pairs)\n",
    "x_train, y_train = pairs[:, 0], pairs[:, 1]\n",
    "\n",
    "# Build and train model\n",
    "inp = tf.keras.Input(shape=(1,))\n",
    "x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inp)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "out = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)\n",
    "model = tf.keras.Model(inp, out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr))\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "\n",
    "# Get word vector\n",
    "embeddings = model.layers[1].get_weights()[0]\n",
    "get_vector = lambda word: embeddings[tok.word_index[word]] if word in tok.word_index else None\n",
    "word = \"football\"\n",
    "print(f\"Vector representation of '{word}': {get_vector(word)}\")\n",
    "\n",
    "# Get context words\n",
    "def get_context_words(word):\n",
    "    if word not in tok.word_index: return []\n",
    "    idx = tok.word_index[word]\n",
    "    context_ids = range(max(1, idx - window_size), min(vocab_size, idx + window_size + 1))\n",
    "    return [w for w, i in tok.word_index.items() if i in context_ids]\n",
    "\n",
    "focus_word = \"playing\"\n",
    "print(f\"Context words for '{focus_word}': {get_context_words(focus_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb31a0-00ca-418a-b672-dc842201d4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
